    There are several other commands
        zookeeper-server-stop.sh
        kafka-server-stop.sh
        kafka-topics.sh
        kafka-console-producer.sh
        kafka-console-consumer.sh
        There are bunch of other commands as well.
    In actual implementation we will not use these commands to manage topics or produce/consume messages from topic. However these commands come handy for quick validations and troubleshooting certain issues related to topics.
    We will manage topics, publish messages into topic as well as consume messages from topic programmatically, which means we will be using respective APIs in programming languages such as Java, Scala, Python etc.
    Each of these commands use different details to connect to the cluster. kafka-topics.sh uses zookeeper while other 2 uses kafka broker details.
    Create topic: kafka-topics.sh --zookeeper localhost:2181 --create --topic test --partitions 1 --replication-factor 1
    When we create topic, it will create directory under /tmp/kafka-logs (review log.dirs under server.properties) using the name of topic and partition index (e.g.: test-0)
    List topic: kafka-topics.sh --zookeeper localhost:2181 --list
    Publish Messages: kafka-console-producer.sh --broker-list localhost:9092 --topic test
    From other window Consume Messages: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
    Deleting topic: kafka-topics.sh --zookeeper localhost:2181 --delete --topic test

Validate with gen_logs traffic

Now let us validate using data generated by gen_logs application streaming into the topic by using command line approach (available under bin directory of kafka).


    Make sure zookeeper and kafka broker are running (using telnet command)
    Make sure gen_logs are generating log messages simulating visitor traffic
    We will pipe the output of tail_logs.sh to kafka-console-producer.sh command and then use kafka-console-consumer.sh to consume message from the topic.
    Topic Details
        Name: retail
        Partitions: 3
        Replication Factor: 1 (as kafka broker is only running on one node, we cannot have replication factor higher than 1)
    Create topic: kafka-topics.sh --zookeeper localhost:2181 --create --topic retail --partitions 3 --replication-factor 1
    List topic: kafka-topics.sh --list --zookeeper localhost:2181
    Publish Messages: tail_logs.sh|kafka-console-producer.sh --broker-list localhost:9092 --topic retail
    From other window Consume Messages: kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic retail --from-beginning

Kafka on multi-node cluster

Before going into the details about Apache Kafka, let us review our Big Data developer labs where we have multi node Kafka broker setup along with other Big Data tools such as Hadoop, Spark etc.



    Cluster is setup using Hortonworks distribution, which means we can preview the cluster using web UI called as Ambari.
    Cluster topology
        Gateways – gw02.itversity.com and gw03.itversity.com
        Zookeeper Ensemble – nn01.itversity.com, nn02.itversity.com and rm01.itversity.com
        Kafka brokers – wn01.itversity.com to wn05.itversity.com
        Other Services – Hadoop, Spark, HBase etc.
    Topic
        Distributed log file (distribution is achieved via partitions)
        Each partition is nothing but directory named after topic and appended by partition index
        In a multinode cluster we use higher replication-factor so that we can have multiple copies of each of the partition for fault tolerance.
        Each of these copies are managed by brokers. As there will be multiple brokers associated with each partition of a given topic, one of the broker will be designated as leader.


kafka-topics.sh \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --create \
  --topic retail \
  --partitions 3 \
  --replication-factor 2
  
kafka-topics.sh \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --list \
  --topic retail
  
kafka-topics.sh \
  --zookeeper nn01.itversity.com:2181,nn02.itversity.com:2181,rm01.itversity.com:2181 \
  --describe \
  --topic retail
  
tail_logs.sh|kafka-console-producer.sh \
  --broker-list wn01.itversity.com:6667,wn02.itversity.com:6667 \
  --topic retail
  
kafka-console-consumer.sh \
  --bootstrap-server wn01.itversity.com:6667,wn02.itversity.com:6667 \
  --topic retail \
--from-beginning



    Producers connect to one or more brokers and push messages to topics via leader.
    Consumers pull message from topic by polling topic at regular intervals. Each time consumer read messages it need to keep track of offset (can be done using multiple ways)
    Role of Zookeeper
        Create and manage topics (a topic is nothing but distributed log file with one or more partitions)
        Zookeeper understand load on the Kafka brokers and take care of creation of partitions as part of nodes on which brokers are running.
        Zookeeper will also assign leader for each of the partition.
        Manage consumer offsets
    Role of Gateway nodes
        Developers can troubleshoot issues from gateway nodes
        Plugins which produce messages to Kafka topic or consume messages from Kafka topic can be deployed on gateway nodes.

Understanding Zookeeper and Kafka Properties

Let us spend some time in understanding Zookeeper as well as Kafka configuration properties. You can do it using command line or UI. If the cluster is setup using distributions such as Hortonworks or Cloudera, we can use Ambari or Cloudera Manager to review the properties.


    For local installation – you can go to $KAFKA_HOME/config directory and visit zookeeper.propeties and server.properties to review some of the important properties.
    For multinode installation, you will have these files on all the nodes and hence reviewing them via command line is not reliable.
    Whatever management tools you have you should be able to review the properties through their interfaces. In our case we will review the cluster managed by Ambari
    You can go to zookeeper configs and kafka configs to review the properties
    Also let us understand some of the important properties with respect to zookeeper and kafka



    Apache Kafka is an open-source stream-processing software platform developed by the Apache Software Foundation, written in Scala and Java. It was initially developed as an internal product at LinkedIn and was open-sourced and adopted by apache foundation.
    Named after author Franz Kafka
    Salient Features:
        Highly Scalable (partitioning)
        Fault Tolerant (replication factor)
        Low Latency
        High Throughput

Kafka eco system

Heart of Kafka is topic a distributed and fault tolerant log file. However over a period of time Kafka is evolved into eco system of tools.

    Kafka Connect
    Kafka Streams and Kafka SQL
    Producer and Consumer APIs
    3rd party plugins to integrate with Flume, logstash, Spark Streaming, Storm, Flink etc.


Kafka Use cases

As micro services have evolved Kafka become popular to integrate data between different micro services – asynchronous, real time as well as batch.

    Activity Tracking: Kafka was originally developed for tracking user activity on LinkedIn
    Messaging: Kafka is also used for messaging, where applications need to send notifications (such as emails) to users.
    Metrics and logging: Applications publish metrics on a regular basis to a Kafka topic, and those metrics can be consumed by systems for monitoring and alerting.
    Commit log: database changes can be published to Kafka and applications can easily monitor this stream to receive live updates as they happen. This changelog stream can also be used for replicating database updates to a remote system.
    Stream processing: Kafka can be integrated with stream frameworks such as Spark Streaming, Flink, Storm etc. Users are allowed to write applications to operate on Kafka messages, performing tasks such as counting metrics, transform data, etc.


Glossary

Topic: A topic represent group of files and directories. When we create topic, it will create directories with topic name and partition index. These directories have bunch of files which will actually store the messages that are being produced.

Publisher or Producer: Publishers or producers are processes that publish data (push messages) to the log file associated with Kafka topic.

Subscriber or Consumer: Subscribers or consumers are processes that read from the log file associated with Kafka topic


Partition: Kafka topics are divided into a number of partitions, which contains messages in an unchangeable sequence. This allows for multiple consumers to read from a topic in parallel.

Leader: When we create Kafka topic with partitions and replication factor, each partition will have leader. Messages will be first written to the partition on broker which is designated as leader and then copied to rest of followers.

Replication Factor: Each partition can be cloned into multiple copies using replication factor. It will facilitate fault tolerance. With replication factor of n on m node cluster (where n <= m), cluster can survive the failure of n-1 nodes at any point in time.

Broker: A Kafka cluster consists of one or more servers (Kafka brokers), which are running Kafka. Producers query metadata of each of the topic and connect to leader of each partition to produce messages into Kafka topic. Consumers do the same while consuming messages from the topic.

Offset: The records in the partitions are each assigned a sequential id number called the offset that uniquely identifies each record within the partition.








