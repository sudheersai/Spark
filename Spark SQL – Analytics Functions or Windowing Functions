Development Life Cycle (daily product revenue)

Let us develop the application using Pycharm and run it on the cluster.

    Make sure application.properties have required input path and output path along with execution mode
    Read orders and order_items data into data frames
    Filter for complete and closed orders
    Join with order_items
    Aggregate to get revenue for each order_date and order_item_product_id
    Sort in ascending order by date and then descending order by revenue
    Save the output as CSV format
    Validate using Pycharm
    Ship it to the cluster, run it on the cluster and validate.



-------------------------------------------------------------

[dev]
executionMode = local
input.base.dir = /Users/itversity/Research/data/retail_db
output.base.dir = /Users/itversity/Research/data/bootcamp/pyspark

[prod]
executionMode = yarn-client
input.base.dir = /public/retail_db
output.base.dir = /user/training/bootcamp/pyspark

-----------------------------------------------------------------

import configparser as cp, sys
from pyspark.sql import SparkSession

props = cp.RawConfigParser()
props.read('src/main/resources/application.properties')
env = sys.argv[1]

spark = SparkSession.\
    builder.\
    appName("Daily Product Revenue using Data Frames and Spark SQL").\
    master(props.get(env, 'executionMode')).\
    getOrCreate()

spark.conf.set('spark.sql.shuffle.partitions', '2')

inputBaseDir = props.get(env, 'input.base.dir')
ordersCSV = spark.read. \
  csv(inputBaseDir + '/orders'). \
  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')

orderItemsCSV = spark.read. \
  csv(inputBaseDir + '/order_items'). \
  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id',
       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')

from pyspark.sql.types import IntegerType, FloatType

orders = ordersCSV. \
  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \
  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))

orderItems = orderItemsCSV.\
    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \
    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \
    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \
    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \
    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \
    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))

orders.createTempView('orders')
orderItems.createTempView('order_items')

spark.conf.set('spark.sql.shuffle.partitions', '2')

dailyProductRevenue = spark.sql('''select o.order_date, oi.order_item_product_id, 
             round(sum(oi.order_item_subtotal), 2) as revenue
             from orders o join order_items oi
             on o.order_id = oi.order_item_order_id
             where o.order_status in ("COMPLETE", "CLOSED")
             group by o.order_date, oi.order_item_product_id
             order by o.order_date, revenue desc''')

outputBaseDir = props.get(env, 'output.base.dir')
dailyProductRevenue.write.csv(outputBaseDir + '/daily_product_revenue_sql')

-------------------------------------------------------------------------------

spark-submit --master yarn \
  --deploy-mode client \
  --conf spark.ui.port=12901 \
  src/main/python/retail_db/df/DailyProductRevenueDFS.py \
prod

------------------------------------------------------

Aggregations, Ranking and Windowing Functions

Let us understand APIs related to aggregations, ranking and windowing functions.

    There are multiple clauses with in SQL to accomplish these
        over
        partition by
        order by
    All aggregate functions, rank functions and windowing functions can be used with over clause to get aggregations per partition or group
    It is mandatory to specify over clause
    e.g.: rank() over(spec) where spec can be partition by or order by or both
    Aggregations – sum, avg, min, max etc
    Ranking – rank, dense_rank, row_number etc
    Windowing – lead, lag etc
    We typically use partition by clause for aggregations and then partition by as well as order by for ranking and windowing functions.


----------------------------------------------------------------

orderItemsCSV = spark.read. \
  csv('/public/retail_db/order_items'). \
  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id', 
       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')

from pyspark.sql.types import IntegerType, FloatType

orderItems = orderItemsCSV.\
    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \
    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \
    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \
    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \
    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \
    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))

orderItems.createTempView('order_items')


spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal,
             round(sum(oi.order_item_subtotal) over (partition by oi.order_item_order_id), 2) order_revenue
             from order_items oi
          ''').show()

spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal, 
             rank() over 
                (partition by oi.order_item_order_id 
                 order by oi.order_item_subtotal desc
                ) rnk
             from order_items oi
          ''').show()


spark.sql('''select oi.order_item_id, oi.order_item_order_id, oi.order_item_subtotal,
             lead(oi.order_item_subtotal) 
                  over (partition by oi.order_item_order_id 
                  order by oi.order_item_subtotal desc
                 ) next_order_item_subtotal
             from order_items oi
          ''').show()

----------------------------------------------------------------

Problem Statement – Get top n products per day

Let us define the problem statement and see the real usage of analytics function.

    Problem Statement – Get top N Products Per day
    Get daily product revenue code from previous topic
    Use ranking functions and get the rank associated based on revenue for each day
    Once we get rank, let us filter for top n products.

Understanding over, partition by and order by clauses

Let us understand different clauses required for analytics functions.

    Typical syntax – function(argument) over (partition by groupcolumn [order by [desc] ordercolumn])
    For aggregations we can define group by using partition by
    For ranking or windowing we need to use partition by and then order by. partition by is to group the data and order by is to sort the data to assign rank.
    We will not be able to use these any where except for select clause
    If we have to filter on these derived fields in select clause, we need to nest the whole query into another query.

Performing aggregations

Let us see how to perform aggregations with in each group.

    We have functions such as sum, avg, min, max etc which can be used to aggregate the data.
    We need to use over (partition by) to get aggregations with in each group.
    Some realistic use cases
        Get average salary for each department and get all employee details who earn more than average salary
        Get average revenue for each day and get all the orders who earn revenue more than average revenue
        Get highest order revenue and get all the orders which have revenue more than 75% of the revenue

Using windowing functions

Let us see details about windowing functions with in each group

    We have functions such as lead, lag etc
    We need to use partition by and then order by for most of the windowing functions
    Some realistic use cases
        Salary difference between current and next/previous employee with in each department

Ranking with in each partition or group

Let us talk about ranking functions with in each group.

    We have functions like rank, dense_rank, row_number, first, last etc
    We need to use partition by and then order by for most of the windowing functions
    Some realistic use cases
        Assign rank to employees based on salary with in each department
        Assign ranks to products based on revenue each day or month

-----------------------------------------------------------------------------------------

Production code :

import configparser as cp, sys
from pyspark.sql import SparkSession

props = cp.RawConfigParser()
props.read('src/main/resources/application.properties')
env = sys.argv[1]

spark = SparkSession.\
    builder.\
    appName("Daily Product Revenue using Data Frames and Spark SQL").\
    master(props.get(env, 'executionMode')).\
    getOrCreate()

spark.conf.set('spark.sql.shuffle.partitions', '2')

inputBaseDir = props.get(env, 'input.base.dir')
ordersCSV = spark.read. \
  csv(inputBaseDir + '/orders'). \
  toDF('order_id', 'order_date', 'order_customer_id', 'order_status')

orderItemsCSV = spark.read. \
  csv(inputBaseDir + '/order_items'). \
  toDF('order_item_id', 'order_item_order_id', 'order_item_product_id',
       'order_item_quantity', 'order_item_subtotal', 'order_item_product_price')

from pyspark.sql.types import IntegerType, FloatType

orders = ordersCSV. \
  withColumn('order_id', ordersCSV.order_id.cast(IntegerType())). \
  withColumn('order_customer_id', ordersCSV.order_customer_id.cast(IntegerType()))

orderItems = orderItemsCSV.\
    withColumn('order_item_id', orderItemsCSV.order_item_id.cast(IntegerType())). \
    withColumn('order_item_order_id', orderItemsCSV.order_item_order_id.cast(IntegerType())). \
    withColumn('order_item_product_id', orderItemsCSV.order_item_product_id.cast(IntegerType())). \
    withColumn('order_item_quantity', orderItemsCSV.order_item_quantity.cast(IntegerType())). \
    withColumn('order_item_subtotal', orderItemsCSV.order_item_subtotal.cast(FloatType())). \
    withColumn('order_item_product_price', orderItemsCSV.order_item_product_price.cast(FloatType()))

orders.createTempView('orders')
orderItems.createTempView('order_items')

spark.conf.set('spark.sql.shuffle.partitions', '2')

dailyProductRevenue = spark.sql('''select o.order_date, oi.order_item_product_id, 
             round(sum(oi.order_item_subtotal), 2) as revenue
             from orders o join order_items oi
             on o.order_id = oi.order_item_order_id
             where o.order_status in ("COMPLETE", "CLOSED")
             group by o.order_date, oi.order_item_product_id
             order by o.order_date, revenue desc''')

dailyProductRevenue.createTempView('daily_product_revenue')

topNDailyProducts = spark.sql('''select q.order_date, q.order_item_product_id, q.revenue
             from (select order_date, order_item_product_id, revenue,
                   rank() over (partition by order_date order by revenue desc) rnk
                   from daily_product_revenue) q
             where q.rnk <= %s
             order by q.order_date, q.revenue desc''' % topN)

outputBaseDir = props.get(env, 'output.base.dir')
topNDailyProducts.write.csv(outputBaseDir + '/topn_daily_products_dfs')



spark-submit --master yarn \
  --deploy-mode client \
  --conf spark.ui.port=12901 \
  src/main/python/retail_db/df/TopNDailyProductsDFS.py \
prod 5

